---
---

@misc{li2024pctoolkit,
      title={PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models}, 
      author={Jinyi Li and Yihuai Lan and Lei Wang and Hao Wang},
      year={2024},
      eprint={2403.17411},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      bibtex_show={true},
      preview={logo_trans.png},
      abstract={Prompt compression is an innovative method for efficiently condensing input prompts while preserving essential information. To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is a unified plug-and-play solution for compressing prompts in Large Language Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and metrics for comprehensive performance evaluation. PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces. In this paper, we outline the key components and functionalities of PCToolkit. We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, summarization, mathematical problem-solving, question answering, few-shot learning, synthetic tasks, code completion, boolean expressions, multiple choice questions, and lies recognition. },
      pdf={https://arxiv.org/pdf/2403.17411.pdf},
      arxiv={2403.17411},
      google_scholar_id={Js6FtkMAAAAJ},
      doi={10.48550/arXiv.2403.17411},
      dimensions={true}
}
