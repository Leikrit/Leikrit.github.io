<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mountain Car Continuous | Jinyi LI </title> <meta name="author" content="Jinyi LI"> <meta name="description" content="A DDPG-based RL method."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Newsreader:opsz,wght@6..72,200..800&amp;family=Roboto:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%87&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://leikrit.github.io/projects/14_DDPG/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jinyi LI </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/people/">connections</a> <a class="dropdown-item " href="https://trusting-heron-f61.notion.site/136e49071c6180b08f82f886a218f76d" rel="external nofollow noopener" target="_blank">notion space</a> <a class="dropdown-item " href="/ziying/">rendezvous</a> </div> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mountain Car Continuous</h1> <p class="post-description">A DDPG-based RL method.</p> </header> <article> <p>This project was completed by myself.</p> <h3 id="abstract">Abstract</h3> <p>This model is based on <strong><em>DDPG</em></strong> Reinformcement Learning Algorithm. Our goal is let the car go upon the hill to reach the flag. However, the momentum of the car cannot support the car to go directly to the top at a time. Thus, the car need to use the hill on the left to accelerate itself to rush upon the right hill to reach the flag.</p> <p>The main problem is that the penalty of movement may cause the car to stay still and not taking any action. This is because the positive reward is too difficult to reach and the penalty of exploring is too high. Generally, if the car never reaches the flag, it will eventually choose to stay still.</p> <p>My solution to solve this problem is adding a descending random propelling (which is considered as <strong><em>noise</em></strong>) to the action that the neural network is generating. By doing this, we can literally “propel” the car to go, so that it cannot stay still.</p> <h3 id="mountain-car-mdp">Mountain Car MDP</h3> <p>The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. There are two versions of the mountain car domain in gymnasium: one with discrete actions and one with continuous. We will experiment with the continouse version in this one.</p> <p><img src="https://gymnasium.farama.org/_images/mountain_car_continuous.gif" alt="Mountain Car"></p> <p>The observation is a ndarray with shape (2,) where the elements correspond to the position of the car along the x-axis and the velocity of the car.</p> <p>The action is a ndarray with shape (1,), representing the directional force applied on the car. The action is clipped in the range [-1,1] and multiplied by a power of 0.0015.</p> <p>A negative reward of $-0.1 * action^2$ is received at each timestep to penalise for taking actions of large magnitude. If the mountain car reaches the goal then a positive reward of +100 is added to the negative reward for that timestep.</p> <p>You can read more the MountainCar Continous environment <a href="https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/" rel="external nofollow noopener" target="_blank">here</a></p> <h4 id="continuous-action-spaces">Continuous Action Spaces</h4> <p>It’s conceptually the same as for discrete action spaces. In discrete action spaces you have a discrete distribution like the categorical distribution and you calculate the log pi probabilities using its mass function. In continuous action spaces you use a continuous distribution like the Gaussian distribution and calculate the log pi “probabilities” using its density function. Everything else is the same.</p> <p>So instead of having network that outputs the parameters for a categorical distribution, you have a network that outputs parameters for (usually) a Gaussian distribution (i.e. the mean and standard deviation)</p> <p>But, usually we just output the mean using a neural network, and have a separate set of learnable parameters for std (in log space) for each action. For example, if we 3 continous actions then we will have a neural network which will output the means for these actions and then separate 3 parameters which will represent the log std of these actions. We still learn these log std but they are not dependent on state. We just have a log std for each action.</p> <p>Other approaches for std include:</p> <ul> <li>Having a std as output of neural network just as the mean.</li> <li>Initializing some std for all actions at start and then steadily decrease if using a std_decay hyperparameter as the training progresses.</li> </ul> <h3 id="deep-deterministic-policy-gradient-ddpg-algorithm">Deep Deterministic Policy Gradient (DDPG) Algorithm</h3> <p>The stochastic policy gradient calculating formula based on Q value is: \(\nabla_\theta J(\pi_\theta) = E_{s\sim\rho ^ \pi,a\sim \pi_\theta}[\nabla_\theta \log \pi_\theta (s,a)Q_\pi(s,a)]\)</p> <p>The deterministic policy gradient calculating formula based on Q value is: \(\nabla_\theta J(\pi_\theta) = E_{s\sim\rho ^ \pi}[\nabla_\theta \pi_\theta (s) \nabla_a Q_\pi(s,a)|_{a=\pi_\theta (s)}]\)</p> <h4 id="ddpg-brief-principle">DDPG Brief Principle</h4> <p>Very generally speaking, DDPG Algorithm is a combination of DQN Algorithm and Actor-and-Critic Algorithm. Where both actor network and critic network are separated into policy network and target network. So, there will be 4 neural network in total.</p> <h4 id="more-specifically">More Specifically…</h4> <p>DDPG has 4 networks: Actor Current network, Actor target network, Critic Current network, Critic target network. The structure of the two Actor networks is the same, and the structure of the two Critic networks is the same.</p> <p>The current Q network of DQN is responsible for using <strong>ϵ− greedy</strong> method to select the action \(A\) for the current state \(S\), perform the action \(A\), obtain the new state \(S'\) and reward \(R\), put the sample into the experience <strong><em>replay pool</em></strong>, and use greedy method to select the action \(A\) for the next state \(S'\) sampled in the experience replay pool, so that the target Q network can calculate the target Q value. After the target Q network has calculated the target Q value, The current Q network updates network parameters and periodically copies the latest network parameters to the target Q network.</p> <p>By analogy with DQN, the current network of Critic in DDPG, the target network of Critic and the current Q network of DQN, the functional positioning of the target Q network is basically similar. However, there is no need to be greedy when selecting an action, and it is done with the Actor current network. The greedy method is used to select the action \(A\) for the next state \(S'\) sampled in the experience replay pool. This part of the work is used to estimate the target Q value, so it can be completed in the Actor target network.</p> <ul> <li> <p><strong>Actor Current network</strong>: Responsible for the iterative update of policy network parameters \(\theta\), responsible for selecting the current action \(A\) according to the current state \(S\), for interacting with the environment to generate \(S'\) and \(R\).</p> </li> <li> <p><strong>Actor Target Network</strong>: Responsible for selecting the optimal next action \(A\) based on the next state \(S'\) sampled in the experiential replay pool. Network parameters are periodically replicated.</p> </li> <li> <p><strong>Critic Current network</strong>: Responsible for the iterative update of the value network parameter \(w\), responsible for calculating the current Q value \(Q(S,A,w)\).</p> </li> <li> <p><strong>Critic Target network</strong>: Responsible for calculating the target Q value. Where \(Q' = R + \gamma Q'(S', A', w')\)</p> </li> </ul> <p>Additionally, the way DDPG copies the current network parameters to the target network has changed.</p> <p>In DQN, we copy the parameters of the current Q network directly to the target Q network. (A hard copy)</p> <p>But now we need a soft copy to update the parameters.: \(w' \leftarrow \tau w + (1-\tau)w'\)</p> \[\theta' \leftarrow \tau \theta + (1-\tau)\theta'\] <p>At the same time, in order to add some randomness to the learning process and increase the coverage of learning, DDPG will add some noise \(N\) to the selected actions, that is, the expression of the final action \(A\) interacting with the environment is:</p> \[A = \pi_\theta(s) + N\] <p>And this is considered to be the <em>propelling</em>.</p> <p>For the Critic Current network, the loss function is basically the MSE loss.</p> <p>For the Actor Current network, the loss gradient is: \(\nabla_J (\theta) = \frac{1}{m}\sum_{j=1} ^ m [\nabla_a Q(s_i,a_i,w)|_{s=s_i,a=\pi_\theta(s)}\nabla_\theta \pi_\theta (s)|_{s=s_i}]\)</p> <h3 id="actor-neural-network">Actor Neural Network</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Actor</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear3</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <h3 id="critic-neural-network">Critic Neural Network</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Critic</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_size</span> <span class="o">+</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear3</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <h3 id="ddpg-agent">DDPG Agent</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
         <span class="c1"># Actor Network and Target Network
</span>        <span class="n">self</span><span class="p">.</span><span class="n">actor</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_target</span> <span class="o">=</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

        <span class="c1"># Critic Network and Target Network
</span>        <span class="n">self</span><span class="p">.</span><span class="n">critic</span> <span class="o">=</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_target</span> <span class="o">=</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

        <span class="c1"># copy weights
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">hard_update</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor_target</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">hard_update</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">critic_target</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nc">Memory</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="n">self</span><span class="p">.</span><span class="n">std</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># A parameter that will descend to calculate the noise
</span>
    <span class="k">def</span> <span class="nf">hard_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">network</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">soft_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">network</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
            <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">tau</span><span class="o">*</span><span class="n">param</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">tau</span><span class="p">)</span><span class="o">*</span><span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>

        <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">next_state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">reward</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">action</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">done</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">int</span><span class="p">()</span>

        <span class="c1"># update critic
</span>        <span class="n">next_action</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor_target</span><span class="p">(</span><span class="n">next_state</span><span class="p">)</span>

        <span class="n">Q_target</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">critic_target</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="n">next_action</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
        <span class="n">Q_target</span> <span class="o">=</span> <span class="n">reward</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">Q_target</span><span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">done</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>


        <span class="n">critic_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">critic</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">),</span> <span class="n">Q_target</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">critic_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">critic_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># update actor
</span>
        <span class="n">action_prediction</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">actor_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="nf">critic</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action_prediction</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>


        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">actor_loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># update actor_target and critic_target
</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">soft_update</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">critic_target</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">soft_update</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor_target</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">actor</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">noise</span><span class="p">:</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">std</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">action</span> <span class="o">+</span> <span class="n">noise</span>

        <span class="k">if</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">push</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">sample</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">actor</span><span class="p">,</span> <span class="sh">"</span><span class="s">actor.pkl</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">critic</span><span class="p">,</span> <span class="sh">"</span><span class="s">critic.pkl</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">new_env</span> <span class="o">=</span> <span class="nf">make</span><span class="p">(</span><span class="sh">"</span><span class="s">MountainCarContinuous-v0</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">new_env</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">new_env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
            <span class="n">local_reward</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">new_env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="n">local_reward</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="n">reward</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">local_reward</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reward</span>
</code></pre></div></div> <h3 id="training-process">Training Process</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DDPG2-480.webp 480w,/assets/img/DDPG2-800.webp 800w,/assets/img/DDPG2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/DDPG2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Training process. </div> <h3 id="result">Result</h3> <p>The final effect of DDPG is shown as below:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/DDPG3.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Figure 3. Final Result. </div> <p>For more information, see the work in my <a href="https://github.com/Leikrit/LMH_Summer_Programme/tree/main/DDPG" rel="external nofollow noopener" target="_blank">github repository</a>!</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jinyi LI. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 31, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>